{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc5c183",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af495014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc4c7a",
   "metadata": {},
   "source": [
    "### Using GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0396850",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09677e69",
   "metadata": {},
   "source": [
    "### Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449e5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 256\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize((imsize, imsize)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image(img_path):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ba368",
   "metadata": {},
   "source": [
    "### Load content and style images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40dfaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img = load_image(\"content.jpg\")\n",
    "style_img = load_image(\"style.jpg\")\n",
    "\n",
    "assert content_img.shape == style_img.shape, \"Images must be the same size\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e446f",
   "metadata": {},
   "source": [
    "### Inititalize and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33cf1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_img = content_img.clone().requires_grad_(True)\n",
    "\n",
    "normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c284eba",
   "metadata": {},
   "source": [
    "### Define Normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "886219f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = mean.clone().detach().view(-1, 1, 1)\n",
    "        self.std = std.clone().detach().view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f36e4",
   "metadata": {},
   "source": [
    "### Content and style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24971857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super().__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.loss = nn.functional.mse_loss(x, self.target)\n",
    "        return x\n",
    "\n",
    "def gram_matrix(x):\n",
    "    b, c, h, w = x.size()\n",
    "    features = x.view(b * c, h * w)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(b * c * h * w)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super().__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, x):\n",
    "        G = gram_matrix(x)\n",
    "        self.loss = nn.functional.mse_loss(G, self.target)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a347f4",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3afef9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()\n",
    "\n",
    "content_layers = ['conv_4']\n",
    "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4']\n",
    "\n",
    "def get_model_and_losses(cnn, norm_mean, norm_std, style_img, content_img):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    normalization = Normalization(norm_mean, norm_std).to(device)\n",
    "\n",
    "    content_losses, style_losses = [], []\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f'conv_{i}'\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f'relu_{i}'\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f'pool_{i}'\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f'bn_{i}'\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target)\n",
    "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    return model, style_losses, content_losses\n",
    "\n",
    "model, style_losses, content_losses = get_model_and_losses(\n",
    "    cnn, normalization_mean, normalization_std, style_img, content_img\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2fc13d",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e976f8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "Step 50: Style: 0.00, Content: 111.17\n",
      "Step 100: Style: 0.00, Content: 108.70\n",
      "Step 150: Style: 0.00, Content: 109.16\n",
      "Step 200: Style: 0.00, Content: 106.57\n",
      "Step 250: Style: 0.00, Content: 106.06\n",
      "Step 300: Style: 0.00, Content: 105.23\n"
     ]
    }
   ],
   "source": [
    "# Weights\n",
    "style_weight = 1e5\n",
    "content_weight = 1e0\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.LBFGS([input_img])\n",
    "\n",
    "print(\"Optimizing...\")\n",
    "\n",
    "run = [0]\n",
    "while run[0] <= 300:\n",
    "\n",
    "    def closure():\n",
    "        with torch.no_grad():\n",
    "            input_img.clamp_(0, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model(input_img)\n",
    "\n",
    "        style_score = sum(sl.loss for sl in style_losses)\n",
    "        content_score = sum(cl.loss for cl in content_losses)\n",
    "\n",
    "        loss = style_weight * style_score + content_weight * content_score\n",
    "        if torch.isnan(loss):\n",
    "            raise ValueError(\"Loss became NaN\")\n",
    "\n",
    "        loss.backward()\n",
    "        run[0] += 1\n",
    "        if run[0] % 50 == 0:\n",
    "            print(f\"Step {run[0]}: Style: {style_score.item():.2f}, Content: {content_score.item():.2f}\")\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "# Final clamp and save\n",
    "with torch.no_grad():\n",
    "    input_img.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028c7b3",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e807d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as output.png\n"
     ]
    }
   ],
   "source": [
    "unloader = transforms.ToPILImage()\n",
    "image = input_img.cpu().clone().squeeze(0)\n",
    "image = unloader(image)\n",
    "image.save(\"output.png\")\n",
    "print(\"Saved as output.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
